几种常见的最优化方法，Mark一下：

**梯度下降法，最速下降法**

梯度下降法，用来解决线性回归模型的求解问题，要你和函数，F作为损失函数，$\theta$是参数。

**批量梯度下降BGD**

主要是批量计算，能得到全局最优解，$\sum{\frac{\delta{F}(\theta,x_i)}{\delta\theta_j}}$，按照每个$\theta$的方向更新$\theta_j=\theta_j-\eta\sum{\frac{\delta{F}(\theta,x_i)}{\delta\theta_j}}$，用到了所有的数据，计算量很大。

**随机梯度下降SGD**

如用上面的式子，随机梯度下降每次不使用全部样本，只使用一个样本，每次使用随机的数据来提升效率。



**牛顿法**

是用来近似求解方程的方法，使用目标函数的泰勒展开的前几项来寻找根，牛顿法收敛的很快：

令$f(x)$展开$f(x_k+\Delta{x}) = F(x_k)+J\Delta{x}+\frac{1}{2}\Delta{x}^TH(x_k)\Delta{x}$，那么求解的式子就是$H\Delta{x}=-J$，然后更新方程就出来了。

牛顿法是二阶方法，收敛速度快，但是要求目标函数的hessen矩阵比较复杂，如果hessen矩阵是奇异的那么算法就不收敛，求解失败。



**高斯牛顿法**

牛顿法直接求二阶导数hessen矩阵，高斯牛顿法则分解目标函数

高斯牛顿法假定$f(x),J$都是$n\times{1}$的向量，那么在计算$f^TJ=J^Tf$的时候都是标量，随便加减就行。

$f(x+\Delta) = f(x)+J\Delta{x}$，

$\arg\min_{\Delta{x}}\frac{1}{2}{ \|f(x)+J\Delta{x}\| } = \frac{1}{2}(f(x)^2+2f(x)J^T\Delta{x}+\Delta{x}^TJ^TJ\Delta{x})$，

求上面式子关于$\Delta{x}$的导数，$2J^Tf(x)+2J^TJ\Delta{x}=0$，

就得到$J^TJ\Delta{x} = -J^Tf(x)$，将$J^TJ$看作hessen矩阵的话，那么高斯牛顿就和牛顿法是同样的思路了。

相当于用$J^TJ$近似hessen矩阵，但是$J$仍旧会出现为奇异矩阵的情况，导致算法不收敛。



**LM方法**

LM针对高斯牛顿的奇异性，增加了拉格朗日因子。

$\arg\min_{\Delta{x}}\frac{1}{2}{ \|f(x)+J\Delta{x}\| }+\frac{\lambda}{2}{D\Delta{x}}$，

拉格朗日因子法是将目标函数，引入一个微小的参数，将约束条件和原函数联系，使其配成与状态数量相等的方程，保持等式的正定性，

此时，$H\Delta{x}=-J^Tf(x)$就变成

$(H+\lambda{D^TD})\Delta{x}=-J^Tf(x)$，

通常D是单位阵，就变成$(H+\lambda)\Delta{x}=-J^Tf(x)$，

$\lambda$就是阻尼因子，会影响下降速度，太大下降速度就慢，LM引入比例因子

看过matlab上的实现的代码，就是用比例因子来控制阻尼的大小，

$\rho=\frac{f(x+\Delta{x})-f(x)}{J\Delta{x}}$，如果比例因子很小，那说明一阶占比小，非线性比较严重，应该增加阻尼因子，下降速度降低，

如果比例因子很大，那么一阶占比较大，减小阻尼因子，下降速度加快。



高斯牛顿法解决无约束的最优化问题和求根问题：

优化问题，肯定是要使用到二阶导数的也就是海森矩阵H，本身和$f'(x)=0$的目标也一致，将其二阶展开

$f(x_0+\Delta{x})=f(x_0)+f'(x)\Delta{x}+f''(x)\Delta{x^2}$

当$\Delta{x}$趋近零的时候，等价于$f'(x)+f''(x)\Delta{x}=0$，

那么迭代的过程就变成了$\Delta{x} = \frac{f'(x_0)}{f''(x_0)}$

对于求根的问题，不会用到二阶导数，此时将方程展开为一个等式右边为零的方程$f(x)=0$，这就是目标

那么一阶展开，这里肯定不能二阶展开，不然的话就变成关于$\Delta{x}$的二次型了，就不好求这个$\Delta{x}$了。

$f(x+\Delta{x})=f(x)+f'(x)\Delta{x}=0$

当$\Delta{x}$足够小的时候就有迭代公式为

$\Delta{x} = \frac{f(x_0)}{f'(x_0)}$











