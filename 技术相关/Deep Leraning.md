## 一、传统神经网络

- **网络**：使用全连接（Full Connection），中间神经元相互连接，也被称为多层感知机（MLP）
- **数据集**：常用来处理数值数据，对于图像的处理，需要把图像先拉长成一维（长×宽×高个特征）进行输入，保证输入为一维格式；标签常用One- hot编码格式
- **参数**
	- Epoch：整个数据集要遍历的轮数
	- Batch：在一轮训练中，每次同步训练的数据量
	- LR（Learning Rate）：学习率大训练的快，单损失函数波动幅度大，不容易收敛；学习率小训练慢，损失函数波动小，容易局部陷入局部最优
	- Dropout：训练过程中随机忽略一些神经元，降低过拟合风险，传统神经网络一般都需要加Dropout，加快训练速度，降低过拟合风险
	- 权值初始化：权值参数初始化常用随机高斯初始化，或者截断的随机高斯初始化；偏置参数一般初始化为0
	- 正则化（Regularizer）：常用L~2~正则，L~2~正则化就是在原本的损失函数的基础上增加所有层所有参数的平方和，主要是为了防止过拟合，一般只对权重参数进行正则化

## 二、CNN

- **网络**：卷积神经网络，由卷积层、池化层等结构组成，输出层用全连接，常用于处理图像数据
- **数据集**：输入需要先指定大小并进行统一
- **参数**
	- 激活函数：增加非线性结构，常用ReLU激活函数，所有带权重参数的层都需要连接一个激活函数，全连接层、卷积层带权重参数，池化层不带权重参数
	- 卷积层：用卷积核对图像进行卷积运算
	- 池化层：将特征图进行压缩，会损失一部分特征信息，因此一般在做完一次池化后，再做卷积时使特征图翻倍，补偿损失的特征细信息
	- FC（Full Connection）：全连接层，接在最后一次池化层后，将特征图进行拉长
	- BN层（Batch Normalization）：类似于标准化，防止梯度消失和梯度爆炸，加上后Loss变化更加稳定
	- 优化器：常用SGD（Stochastic Gradient Descent）、Adam（Adaptive Moment Estimation）、Adagrad（Adaptive Gradient），学习率可设置衰减，使其随着Epoch的增大进行衰减，Adam和Adagrad都是自适应学习率的优化算法
	- Dropout：卷积层可不增加
	- Padding：在进行卷积前对图像周围进行填充，让图像边缘信息利用的更加充分
	- 其余超参数与传统网络一致

## 三、RNN

- **网络**：递归神经网络，由长短时记忆网络（LSTM）、Dropout、全连接层等组成，输出层用全连接，常用于处理时间序列数据和文本数据
- **数据集**
	- 时间序列数据，数据集输入是[batch，时间步长，数据量化长度]；数据做验证时有两种方式，一种是迭代预测（用得到的预测值继续预测下一个），一种是直接顺序预测；

	- 文本数据，需要先进行预处理和量化，建立语料库，然后再输入[batch，词数量，词向量长度]

- **参数**
  - 时间步长（Time step）：在一次训练中，同时会有与世间步长一致数量的输出可作为中间结果，但是有时候我们只取最后一个结果，相当于下一个时间的预测值；对于文本语言，即一次输入词的数量
  - 词嵌入层（Word Embeeding）：建立语料库，文本语言量化，batch是句子数量，时间步长是词数量，数据量化长度是词向量长度；词嵌入层中的词向量权重可以随机生成，也可以用别人训练好的，词向量可训练也可不训练，但是训练起来过拟合风险较高；数据输入到全连接层时，需要将一个batch的数据按顺序连接并拉长再输入
  - 词向量（Word Vector）：将单词或短语映射到实数向量，输入到词嵌入层中，作为词向量的权重，一般直接用别人的语料库，但是通常较大，也可以取自己的单词然后将别人的映射复制过来
  - 全局最大池化层：这里用于减少数据量，将一个词的量化长度变成一维，一维上进行池化
  - LSTM：后面如果再接LSTM，则输出序列；如果接全连接层就输出值，一般使用LSTM就不加全局最大池化层，LSTM相当于替代池化层
  - 卷积层：也可以使用卷积层，使用一维卷积核进行计算，卷积核以n个词进行卷积
  - 其余超参数与传统网络一致

## 四、GAN

- **网络**：生成对抗网络，由生成器和判别器组成，生成器网络与判别器网络同时交替训练，
- **数据集**：生成器的输入是噪声数据，判别器的输入和给定的真实值和生成器的输出，标签是真和假
- **生成器**：输出层输出数据的格式必须同需要的一致，输出层激活函数常用tanh函数将数据压缩到-1到+1之间，生成器进行训练时，需要和判别器连在一起，但是不更新判别器的权值，例如对于图像生成而言，该组合的输入是噪音数据，之后生成器生成图像，之后由判别器给出判别的真假结果
- **判别器**：看作为二分类网络即可，网络输出部分使用Sigmoid函数，损失函数使用binary_crossentropy，使用传统神经网络或者卷积神经网络均可练时
- **训练过程**：生成器网络与判别器网络同时交替训练。首先训练判别器，拿一批真实数据与一批噪声，噪声输入给生成器生成一批假数据，然后将真数据和假数据都输入给判别器，此时损失函数由真实数据训练的损失和假数据训练的损失两部分相加组成，计算并更新判别器的权值。然后训练生成器，此时需要将生成器和判别器组合起来对生成器进行训练，此时损失函数是看噪声数据输入后判别器最终给出真的损失函数。训练过程中，判别器的准确度越接近50%训练的越好。
- **DCGAN**
	- 若GAN网络的生成器和判别器都使用传统的全连接神经网络，则生成器直接输入序列噪声即可，判别器的真实数据输入需要先拉直成一维，生成器输出格式需要转换成所需要的格式
	- DCGAN使用卷积网络，使用反卷积进行上采样，生成器需要先把噪声序列转换成多维度数据，之后按照常规卷积的逆过程，特征图长宽变大，维度变小，依次进行上采样，判别器如同常规的卷积网络，最后得到二分类结果

## 五、迁移学习

- **网络**：使用别人训练好的网络作为预训练网络模型，对自己的数据集进行迁移，适合数据量少的数据集
	- **方法**：一种是将原来训练好的的参数作为初始化参数进行继续训练；另一种是冻住部分卷积层参数，只更新另一部分参数，进行微调，冻住多少层根据自己数据量来定，数据集越小冻住的越多。两种方式输出部分的全连接层都需要改变和更新权重参数。想要设定某些参数冻住，即设定相应层不训练即可
	- **参数迁移**：将别人训练好的模型参数读取后作为训练的初始参数，全连接层按照自己要求变更，之后进行训练
	- **效果**：使用别人参数后模型收敛比较快

## 六、Transformer



## **七、多目标分类**

- **多标签网络**：一个网络模型，一个损失函数，得到多个标签结果
	- 标签变成多个二分类的形式，即是与不是各个特征，对每一个位置进行二分类
	- 对于单标签多分类，网络输出部分常用Softmax；对于多标签，由于是对每一个位置进行二分类，因此网络输出部分使用Sigmoid
	- 对于单标签多分类，损失函数常用crossentropy；对于多标签，由于是对每一个位置进行二分类，损失函数使用binary_crossentropy
- **多输出网络**：同样一批数据，训练多个网络，每个网络有分别的损失函数，得到不同的结果，最后保存在一个模型中
	- 输入时同一个，但是搭建两个网络分支，每一个网络都是一个多分类网络，有自己的损失函数，最后可以利用多个损失函数算一个加权的总损失函数

## 八、常用网络

### U-Net

- U-Net网络是一种在生物医学领域中进行图像分割的先进网络模型。该网络结构包括编码器、解码器和瓶颈层三个部分。编码器通过卷积和池化操作逐步减小特征图的尺寸，而解码器通过上采样和特征映射级联操作拼接特征图，并最终得到与原输入图像大小相同的分割结果。U-Net的核心思想是引入跳跃连接，这大大提高了图像分割的精度。通过U-Net网络，可以使用较少的训练样本对像素点进行预测，并对像素点进行着色绘图。该网络模型形状类似字母“U”，因此被称为U-Net。

### ResNet

- 为解决网络深度太深导致参数更新不动，有些层没起到作用或者起到相反作用的问题，增加了残差网络结构，在残差结构中将输入影射到模块的输出，使网络可以加到更深。映射方式有两种 ，一种是深度不变直接映射，一种是进行1×1卷积进行升维映射。
- 主要由conv_block模块和identity_block模块组成，conv_block是进行1×1卷积进行升维映射，而identity_block是深度不变直接映射

### Seq2Seq

- Seq2Seq（Sequence to Sequence，序列到序列模型） 是一种循环神经网络的变种，包括编码器 (Encoder) 和解码器 (Decoder) 两部分。突破了传统的固定大小输入问题框架，将一个作为输入的序列映射为一个作为输出的序列，这一过程由编码（Encoder）输入与解码（Decoder）输出两个环节组成, 前者负责把序列编码成一个固定长度的向量，这个向量作为输入传给后者，输出可变长度的向量

## 数据增强

- 工具包：OpenCV或者Keras.preprocessing.image
- 方法：旋转、平移变换、缩放、翻转、channel_shift、rescale（像素值归一化变换）
- 常用填充办法：constant（常量）、nearest（邻近点）、reflect、wrap

## 其他

BP神经网络

MLP

FCN

## 参考

[Keras API](https://keras.io/api/ "https://keras.io/api/")

[唐宇迪Keras实践教程](https://www.bilibili.com/video/BV1p34y197ha/?spm_id_from=333.1245.0.0&vd_source=13dcf7fb782925cf156eabe37fb05412)